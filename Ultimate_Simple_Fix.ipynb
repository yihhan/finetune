{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîß ULTIMATE SIMPLE FIX - Back to Absolute Basics\n",
        "\n",
        "## üí• **Previous Failures:**\n",
        "- **LoRA + Large Dataset**: 100% different, 0% quality (gibberish)\n",
        "- **Full Fine-Tuning**: Complete failure (\"saskatchewan saskatchewan...\")\n",
        "\n",
        "## üéØ **NEW STRATEGY: Minimal Viable Fine-Tuning**\n",
        "- **Tiny dataset** (5 perfect examples)\n",
        "- **Conservative training** (1 epoch, very low LR)\n",
        "- **Simple model** (T5-small)\n",
        "- **Basic format** (simple input ‚Üí output)\n",
        "\n",
        "## ‚úÖ **Success Criteria:**\n",
        "- Model says \"Monetary Authority of Singapore\" for \"What does MAS stand for?\"\n",
        "- **Quality over everything else**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch peft -q\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "print('üîß ULTIMATE SIMPLE FIX - Proper LoRA Implementation')\n",
        "print('=' * 60)\n",
        "print('Using comprehensive LoRA setup with minimal perfect dataset')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Minimal Perfect Dataset (5 Examples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create minimal perfect dataset\n",
        "minimal_data = [\n",
        "    {\n",
        "        'input': 'What does MAS stand for?',\n",
        "        'output': 'MAS stands for Monetary Authority of Singapore.'\n",
        "    },\n",
        "    {\n",
        "        'input': 'What currency does Singapore use?',\n",
        "        'output': 'Singapore uses the Singapore Dollar (SGD).'\n",
        "    },\n",
        "    {\n",
        "        'input': 'Who regulates banks in Singapore?',\n",
        "        'output': 'The Monetary Authority of Singapore (MAS) regulates banks.'\n",
        "    },\n",
        "    {\n",
        "        'input': 'What is Singapore\\'s central bank?',\n",
        "        'output': 'Singapore\\'s central bank is the Monetary Authority of Singapore (MAS).'\n",
        "    },\n",
        "    {\n",
        "        'input': 'What does SGD stand for?',\n",
        "        'output': 'SGD stands for Singapore Dollar.'\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f'üìä Created minimal dataset: {len(minimal_data)} perfect examples')\n",
        "for i, item in enumerate(minimal_data, 1):\n",
        "    print(f'{i}. {item[\"input\"]} ‚Üí {item[\"output\"]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Comprehensive LoRA Setup (Your Method)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model with comprehensive LoRA setup\n",
        "base_id = \"google/flan-t5-base\"\n",
        "tok = AutoTokenizer.from_pretrained(base_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(base_id)\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(base_id)\n",
        "\n",
        "print(f'‚úÖ Loaded {base_id}')\n",
        "print(f'üìä Parameters: {model.num_parameters():,}')\n",
        "\n",
        "# Comprehensive LoRA configuration (your method)\n",
        "lora_cfg = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=16, \n",
        "    lora_alpha=32, \n",
        "    lora_dropout=0.05, \n",
        "    bias=\"none\",\n",
        "    target_modules=[\n",
        "        \"SelfAttention.q\",\"SelfAttention.k\",\"SelfAttention.v\",\"SelfAttention.o\",\n",
        "        \"DenseReluDense.wi_0\",\"DenseReluDense.wi_1\",\"DenseReluDense.wo\",\n",
        "        \"EncDecAttention.q\",\"EncDecAttention.k\",\"EncDecAttention.v\",\"EncDecAttention.o\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "model.print_trainable_parameters()  # Shows only LoRA params\n",
        "\n",
        "print('‚úÖ Comprehensive LoRA setup complete - targeting ALL key modules!')\n",
        "\n",
        "# Test base model first\n",
        "test_input = 'What does MAS stand for?'\n",
        "inputs = tok(test_input, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = original_model.generate(**inputs, max_new_tokens=20)\n",
        "base_response = tok.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f'üîç Base model response: \"{base_response}\"')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Proper Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare minimal dataset\n",
        "dataset = Dataset.from_list(minimal_data)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['input']\n",
        "    targets = examples['output']\n",
        "    \n",
        "    model_inputs = tok(inputs, max_length=128, truncation=True, padding=True)\n",
        "    labels = tok(targets, max_length=128, truncation=True, padding=True)\n",
        "    \n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "ds_train = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
        "print('‚úÖ Dataset preprocessed')\n",
        "\n",
        "# Proper data collator and training arguments (your method)\n",
        "collator = DataCollatorForSeq2Seq(tokenizer=tok, model=base_id, padding=\"longest\")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"minimal_flan_t5_model\",\n",
        "    learning_rate=5e-4,  # Your higher LR\n",
        "    num_train_epochs=3,  # Your epochs\n",
        "    per_device_train_batch_size=2,  # Smaller for minimal data\n",
        "    gradient_accumulation_steps=2,  # Your gradient accumulation\n",
        "    bf16=torch.cuda.is_available(),  # Mixed precision if available\n",
        "    logging_steps=1,  # Log every step for minimal data\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"no\",\n",
        "    report_to=\"none\",  # No wandb\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, \n",
        "    args=args, \n",
        "    train_dataset=ds_train, \n",
        "    data_collator=collator\n",
        ")\n",
        "\n",
        "print('‚úÖ Proper training setup complete with your comprehensive LoRA method!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with proper LoRA setup\n",
        "print('üöÄ Starting training with comprehensive LoRA setup...')\n",
        "print('Using ALL attention and feed-forward modules!')\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save LoRA adapters properly\n",
        "model.save_pretrained(\"minimal_flan_t5_model/lora_adapters\")\n",
        "print('‚úÖ Training completed and LoRA adapters saved!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Critical Test - Comprehensive LoRA Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('üß™ CRITICAL TEST - Comprehensive LoRA Results')\n",
        "print('=' * 60)\n",
        "\n",
        "test_questions = [\n",
        "    'What does MAS stand for?',\n",
        "    'What currency does Singapore use?',\n",
        "    'Who regulates banks in Singapore?'\n",
        "]\n",
        "\n",
        "success_count = 0\n",
        "device = next(model.parameters()).device\n",
        "original_model = original_model.to(device)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f'\\n{i}. Question: {question}')\n",
        "    \n",
        "    inputs = tok(question, return_tensors='pt').to(device)\n",
        "    \n",
        "    # Base model\n",
        "    original_model.eval()\n",
        "    with torch.no_grad():\n",
        "        base_outputs = original_model.generate(**inputs, max_new_tokens=30)\n",
        "    base_response = tok.decode(base_outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # LoRA fine-tuned model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        ft_outputs = model.generate(**inputs, max_new_tokens=30)\n",
        "    ft_response = tok.decode(ft_outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    print(f'   Base:       \"{base_response}\"')\n",
        "    print(f'   LoRA Fine-tuned: \"{ft_response}\"')\n",
        "    \n",
        "    # Check for success\n",
        "    success_keywords = ['monetary authority', 'singapore dollar', 'mas regulates', 'sgd']\n",
        "    if any(keyword in ft_response.lower() for keyword in success_keywords):\n",
        "        print('   ‚úÖ SUCCESS: Contains expected Singapore content!')\n",
        "        success_count += 1\n",
        "    else:\n",
        "        print('   ‚ùå FAILED: No expected Singapore content')\n",
        "\n",
        "# Final assessment\n",
        "success_rate = (success_count / len(test_questions)) * 100\n",
        "print(f'\\n' + '=' * 60)\n",
        "print(f'üéØ COMPREHENSIVE LORA RESULTS: {success_count}/{len(test_questions)} ({success_rate:.1f}%)')\n",
        "\n",
        "if success_rate >= 66:\n",
        "    print('üéâ SUCCESS: Comprehensive LoRA approach works!')\n",
        "    print('‚úÖ Your method with all modules is effective!')\n",
        "    print('‚úÖ Ready to scale up to larger datasets!')\n",
        "elif success_rate >= 33:\n",
        "    print('‚ö†Ô∏è PARTIAL: Some success, method shows promise')\n",
        "    print('üí° Try: More epochs, different LR, or more data')\n",
        "else:\n",
        "    print('‚ùå FAILED: Even comprehensive LoRA doesn\\'t work')\n",
        "    print('üí° Need: Different model architecture or approach')\n",
        "\n",
        "print(f'\\nüî¨ COMPARISON TO PREVIOUS FAILURES:')\n",
        "print(f'   Previous LoRA (limited modules): 0% quality (gibberish)')\n",
        "print(f'   Full Fine-Tuning: 0% quality (broken repetition)')\n",
        "print(f'   Comprehensive LoRA (all modules): {success_rate:.1f}% quality')\n",
        "\n",
        "if success_rate > 0:\n",
        "    print('\\nüéØ BREAKTHROUGH: First approach to show ANY quality improvement!')\n",
        "    print('‚úÖ This validates your comprehensive LoRA method!')\n",
        "else:\n",
        "    print('\\n‚ö†Ô∏è Still no success - may need fundamental approach change')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
