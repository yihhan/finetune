{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üè¶ Financial Regulation LLM Fine-tuning on Google Colab\n",
        "\n",
        "This notebook demonstrates how to fine-tune a small language model for Singapore financial regulation Q&A using LoRA/QLoRA.\n",
        "\n",
        "## üéØ Project Overview\n",
        "\n",
        "- **Goal**: Replace expensive large-model RAG calls with cost-effective fine-tuned small models\n",
        "- **Domain**: Singapore financial regulations (MAS guidelines, compliance docs)\n",
        "- **Approach**: LoRA fine-tuning for efficient parameter adaptation\n",
        "- **Benefits**: 99.7% cost reduction, local hosting capability, faster responses\n",
        "\n",
        "## üìã Table of Contents\n",
        "\n",
        "1. [Setup and Installation](#setup)\n",
        "2. [Dataset Preparation](#dataset)\n",
        "3. [Model Fine-tuning](#training)\n",
        "4. [Evaluation](#evaluation)\n",
        "5. [Inference Demo](#inference)\n",
        "6. [Results Analysis](#results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers datasets peft accelerate bitsandbytes\n",
        "!pip install nltk rouge-score pandas numpy\n",
        "!pip install beautifulsoup4 requests\n",
        "\n",
        "# Download NLTK data for evaluation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the project repository\n",
        "!git clone https://github.com/yihhan/finetune.git\n",
        "%cd finetune\n",
        "\n",
        "# Check if we have GPU available\n",
        "import torch\n",
        "print(f\"üîß Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected - training will be slower on CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run improved dataset preparation\n",
        "!python improved_dataset_prep.py\n",
        "\n",
        "# Run improved training with better parameters\n",
        "print(\"üöÄ Starting improved model fine-tuning...\")\n",
        "!python improved_train.py\n",
        "\n",
        "# Run improved inference demo\n",
        "print(\"üéØ Testing improved fine-tuned model...\")\n",
        "!python improved_inference.py --demo\n",
        "\n",
        "print(\"‚úÖ Complete pipeline executed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Inspect Enhanced Dataset\n",
        "\n",
        "Quickly preview a few entries and distribution by category.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview enhanced dataset\n",
        "import json, pandas as pd\n",
        "qa_path = \"processed_data/enhanced_financial_regulation_qa.json\"\n",
        "train_path = \"processed_data/enhanced_training_data.json\"\n",
        "\n",
        "with open(qa_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    qa = json.load(f)\n",
        "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    train = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(qa)\n",
        "print(f\"Rows: {len(df)} | Columns: {list(df.columns)}\")\n",
        "print(\"\\nCategory counts:\\n\", df['category'].value_counts())\n",
        "print(\"\\nSample rows:\")\n",
        "print(df.head(3).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Evaluate Fine-tuned vs Base vs RAG\n",
        "\n",
        "Runs `eval.py` and renders a compact table and bar charts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure evaluator points to improved model output\n",
        "import json, os\n",
        "\n",
        "# Run evaluation\n",
        "!python eval.py\n",
        "\n",
        "# Load summary\n",
        "summary_path = \"evaluation_results/summary_metrics.json\"\n",
        "if os.path.exists(summary_path):\n",
        "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        summary = json.load(f)\n",
        "    import pandas as pd\n",
        "    rows = []\n",
        "    for k, pretty in [(\"base_model\",\"Base\"),(\"finetuned_model\",\"Fine-tuned\"),(\"rag_model\",\"RAG (GPT-4)\")]:\n",
        "        if k in summary:\n",
        "            rows.append({\n",
        "                \"Model\": pretty,\n",
        "                \"BLEU\": summary[k][\"avg_bleu\"],\n",
        "                \"ROUGE-1\": summary[k][\"avg_rouge1\"],\n",
        "                \"ROUGE-2\": summary[k][\"avg_rouge2\"],\n",
        "                \"ROUGE-L\": summary[k][\"avg_rougeL\"],\n",
        "                \"Avg Time (s)\": summary[k][\"avg_time\"],\n",
        "            })\n",
        "    df = pd.DataFrame(rows)\n",
        "    print(\"\\nResults summary:\\n\", df.to_string(index=False))\n",
        "else:\n",
        "    print(\"Summary not found at\", summary_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí¨ Inference Viewer (Improved)\n",
        "\n",
        "Ask questions and view answers inline. Uses `improved_inference.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single question inference\n",
        "import subprocess, json, shlex\n",
        "\n",
        "def ask(q):\n",
        "    cmd = f\"python improved_inference.py --question \\\"{q}\\\"\"\n",
        "    print(\"\\nQ:\", q)\n",
        "    print(\"A:\")\n",
        "    try:\n",
        "        out = subprocess.check_output(shlex.split(cmd), stderr=subprocess.STDOUT, text=True)\n",
        "        print(out)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(e.output)\n",
        "\n",
        "ask(\"What are the capital adequacy requirements for banks in Singapore?\")\n",
        "ask(\"How should financial institutions implement AML measures?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Optional: Export Artifacts to Google Drive\n",
        "\n",
        "Save the fine-tuned model, adapters, and results to Drive for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !mkdir -p /content/drive/MyDrive/finreg_llm\n",
        "# !cp -r improved_finetuned_financial_model /content/drive/MyDrive/finreg_llm/\n",
        "# !cp -r evaluation_results /content/drive/MyDrive/finreg_llm/\n",
        "# print(\"Saved to Drive:/MyDrive/finreg_llm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üè¶ Financial Regulation LLM Fine-tuning on Google Colab\n",
        "\n",
        "This notebook demonstrates how to fine-tune a small language model for Singapore financial regulation Q&A using LoRA/QLoRA.\n",
        "\n",
        "## üéØ Project Overview\n",
        "\n",
        "- **Goal**: Replace expensive large-model RAG calls with cost-effective fine-tuned small models\n",
        "- **Domain**: Singapore financial regulations (MAS guidelines, compliance docs)\n",
        "- **Approach**: LoRA fine-tuning for efficient parameter adaptation\n",
        "- **Benefits**: 99.7% cost reduction, local hosting capability, faster responses\n",
        "\n",
        "## üìã Table of Contents\n",
        "\n",
        "1. [Setup and Installation](#setup)\n",
        "2. [Dataset Preparation](#dataset)\n",
        "3. [Model Fine-tuning](#training)\n",
        "4. [Evaluation](#evaluation)\n",
        "5. [Inference Demo](#inference)\n",
        "6. [Results Analysis](#results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup and Installation {#setup}\n",
        "\n",
        "First, let's install all the required dependencies and clone the project repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers datasets peft accelerate bitsandbytes\n",
        "!pip install nltk rouge-score pandas numpy\n",
        "!pip install beautifulsoup4 requests\n",
        "\n",
        "# Download NLTK data for evaluation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the project repository\n",
        "!git clone https://github.com/yihhan/finetune.git\n",
        "%cd finetune\n",
        "\n",
        "# Check if we have GPU available\n",
        "import torch\n",
        "print(f\"üîß Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected - training will be slower on CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Dataset Preparation {#dataset}\n",
        "\n",
        "Let's prepare the Singapore financial regulation dataset for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run improved dataset preparation\n",
        "!python improved_dataset_prep.py\n",
        "\n",
        "# Check what data was created\n",
        "import os\n",
        "print(\"üìÅ Enhanced dataset files created:\")\n",
        "for root, dirs, files in os.walk(\"processed_data\"):\n",
        "    for file in files:\n",
        "        if \"enhanced\" in file:\n",
        "            file_path = os.path.join(root, file)\n",
        "            size = os.path.getsize(file_path)\n",
        "            print(f\"  {file_path} ({size} bytes)\")\n",
        "\n",
        "# Display sample data\n",
        "import json\n",
        "with open(\"processed_data/enhanced_financial_regulation_qa.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "    \n",
        "print(f\"\\nüìä Enhanced Dataset Summary:\")\n",
        "print(f\"  Total Q&A pairs: {len(data)}\")\n",
        "print(f\"  Categories: {set(item['category'] for item in data)}\")\n",
        "\n",
        "print(f\"\\nüìù Sample Q&A:\")\n",
        "sample = data[0]\n",
        "print(f\"Q: {sample['question']}\")\n",
        "print(f\"A: {sample['answer'][:200]}...\")\n",
        "print(f\"Category: {sample['category']}\")\n",
        "\n",
        "# Show training data size\n",
        "with open(\"processed_data/enhanced_training_data.json\", \"r\") as f:\n",
        "    training_data = json.load(f)\n",
        "print(f\"\\nüöÄ Training samples: {len(training_data)} (with augmentation)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
