{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üè¶ Financial Regulation LLM Fine-tuning on Google Colab\n",
        "\n",
        "This notebook demonstrates how to fine-tune a small language model for Singapore financial regulation Q&A using LoRA/QLoRA.\n",
        "\n",
        "## üéØ Project Overview\n",
        "\n",
        "- **Goal**: Replace expensive large-model RAG calls with cost-effective fine-tuned small models\n",
        "- **Domain**: Singapore financial regulations (MAS guidelines, compliance docs)\n",
        "- **Approach**: LoRA fine-tuning for efficient parameter adaptation\n",
        "- **Benefits**: 99.7% cost reduction, local hosting capability, faster responses\n",
        "\n",
        "## üìã Table of Contents\n",
        "\n",
        "1. [Setup and Installation](#setup)\n",
        "2. [Dataset Preparation](#dataset)\n",
        "3. [Model Fine-tuning](#training)\n",
        "4. [Evaluation](#evaluation)\n",
        "5. [Inference Demo](#inference)\n",
        "6. [Results Analysis](#results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers datasets peft accelerate bitsandbytes\n",
        "!pip install nltk rouge-score pandas numpy\n",
        "!pip install beautifulsoup4 requests\n",
        "\n",
        "# Download NLTK data for evaluation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the project repository\n",
        "!git clone https://github.com/yihhan/finetune.git\n",
        "%cd finetune\n",
        "\n",
        "# Check if we have GPU available\n",
        "import torch\n",
        "print(f\"üîß Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected - training will be slower on CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run improved dataset preparation\n",
        "!python improved_dataset_prep.py\n",
        "\n",
        "# Run improved training with better parameters\n",
        "print(\"üöÄ Starting improved model fine-tuning...\")\n",
        "!python improved_train.py\n",
        "\n",
        "# Run improved inference demo\n",
        "print(\"üéØ Testing improved fine-tuned model...\")\n",
        "!python improved_inference.py --demo\n",
        "\n",
        "print(\"‚úÖ Complete pipeline executed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üè¶ Financial Regulation LLM Fine-tuning on Google Colab\n",
        "\n",
        "This notebook demonstrates how to fine-tune a small language model for Singapore financial regulation Q&A using LoRA/QLoRA.\n",
        "\n",
        "## üéØ Project Overview\n",
        "\n",
        "- **Goal**: Replace expensive large-model RAG calls with cost-effective fine-tuned small models\n",
        "- **Domain**: Singapore financial regulations (MAS guidelines, compliance docs)\n",
        "- **Approach**: LoRA fine-tuning for efficient parameter adaptation\n",
        "- **Benefits**: 99.7% cost reduction, local hosting capability, faster responses\n",
        "\n",
        "## üìã Table of Contents\n",
        "\n",
        "1. [Setup and Installation](#setup)\n",
        "2. [Dataset Preparation](#dataset)\n",
        "3. [Model Fine-tuning](#training)\n",
        "4. [Evaluation](#evaluation)\n",
        "5. [Inference Demo](#inference)\n",
        "6. [Results Analysis](#results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup and Installation {#setup}\n",
        "\n",
        "First, let's install all the required dependencies and clone the project repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers datasets peft accelerate bitsandbytes\n",
        "!pip install nltk rouge-score pandas numpy\n",
        "!pip install beautifulsoup4 requests\n",
        "\n",
        "# Download NLTK data for evaluation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the project repository\n",
        "!git clone https://github.com/yihhan/finetune.git\n",
        "%cd finetune\n",
        "\n",
        "# Check if we have GPU available\n",
        "import torch\n",
        "print(f\"üîß Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected - training will be slower on CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Dataset Preparation {#dataset}\n",
        "\n",
        "Let's prepare the Singapore financial regulation dataset for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run improved dataset preparation\n",
        "!python improved_dataset_prep.py\n",
        "\n",
        "# Check what data was created\n",
        "import os\n",
        "print(\"üìÅ Enhanced dataset files created:\")\n",
        "for root, dirs, files in os.walk(\"processed_data\"):\n",
        "    for file in files:\n",
        "        if \"enhanced\" in file:\n",
        "            file_path = os.path.join(root, file)\n",
        "            size = os.path.getsize(file_path)\n",
        "            print(f\"  {file_path} ({size} bytes)\")\n",
        "\n",
        "# Display sample data\n",
        "import json\n",
        "with open(\"processed_data/enhanced_financial_regulation_qa.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "    \n",
        "print(f\"\\nüìä Enhanced Dataset Summary:\")\n",
        "print(f\"  Total Q&A pairs: {len(data)}\")\n",
        "print(f\"  Categories: {set(item['category'] for item in data)}\")\n",
        "\n",
        "print(f\"\\nüìù Sample Q&A:\")\n",
        "sample = data[0]\n",
        "print(f\"Q: {sample['question']}\")\n",
        "print(f\"A: {sample['answer'][:200]}...\")\n",
        "print(f\"Category: {sample['category']}\")\n",
        "\n",
        "# Show training data size\n",
        "with open(\"processed_data/enhanced_training_data.json\", \"r\") as f:\n",
        "    training_data = json.load(f)\n",
        "print(f\"\\nüöÄ Training samples: {len(training_data)} (with augmentation)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
