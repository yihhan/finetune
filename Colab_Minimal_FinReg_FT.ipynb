{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üè¶ Minimal Colab: Fine-tune Small LLM for SG Financial Regulations\n",
        "\n",
        "A streamlined notebook to run the improved pipeline only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Setup\n",
        "!pip install -q torch transformers datasets peft accelerate bitsandbytes\n",
        "!pip install -q nltk rouge-score pandas numpy\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "print('‚úÖ Setup complete')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Clone repo and check GPU\n",
        "!git clone https://github.com/yihhan/finetune.git\n",
        "%cd finetune\n",
        "\n",
        "import torch\n",
        "print('Device:', 'CUDA' if torch.cuda.is_available() else 'CPU')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üìä Dataset Preparation\n",
        "\n",
        "# 3) Enhanced dataset prep + inspection\n",
        "import os, json, pandas as pd\n",
        "qa = 'processed_data/enhanced_financial_regulation_qa.json'\n",
        "tr = 'processed_data/enhanced_training_data.json'\n",
        "\n",
        "if not (os.path.exists(qa) and os.path.exists(tr)):\n",
        "    print(\"üöÄ Generating enhanced dataset...\")\n",
        "    !python improved_dataset_prep.py\n",
        "else:\n",
        "    print('‚úÖ Enhanced dataset exists, skipping generation')\n",
        "\n",
        "# Show dataset details\n",
        "with open(qa, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "with open(tr, 'r', encoding='utf-8') as f:\n",
        "    training_data = json.load(f)\n",
        "\n",
        "print(f\"\\nüìä Dataset Summary:\")\n",
        "print(f\"  Q&A pairs: {len(data)}\")\n",
        "print(f\"  Training samples: {len(training_data)} (with augmentation)\")\n",
        "print(f\"  Categories: {set(item['category'] for item in data)}\")\n",
        "\n",
        "print(f\"\\nüìù Sample Q&A:\")\n",
        "sample = data[0]\n",
        "print(f\"Q: {sample['question']}\")\n",
        "print(f\"A: {sample['answer'][:200]}...\")\n",
        "print(f\"Category: {sample['category']}\")\n",
        "\n",
        "# Category distribution\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"\\nüìà Category distribution:\")\n",
        "print(df['category'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üîç Debug Training Data Quality\n",
        "\n",
        "# 4a) First, check if our training data is actually different from base model\n",
        "print(\"üîç Analyzing training data quality...\")\n",
        "print(\"- Check if data contains Singapore-specific information\")\n",
        "print(\"- Verify data has specific details vs generic responses\")\n",
        "print(\"- Determine if base model would give different answers\")\n",
        "\n",
        "!python debug_training_data.py\n",
        "\n",
        "print(\"üìä Data analysis completed!\")\n",
        "\n",
        "## üí• FULL Fine-tuning (No LoRA)\n",
        "\n",
        "# 4b) Since LoRA keeps failing, try FULL parameter fine-tuning\n",
        "print(\"\\nüí• Starting FULL fine-tuning (no LoRA)...\")\n",
        "print(\"- AGGRESSIVE LoRA still produced identical responses\")\n",
        "print(\"- Try full parameter training on Flan-T5-small\")\n",
        "print(\"- All parameters trainable (not just LoRA adapters)\")\n",
        "print(\"- Limited to 20 samples to prevent overfitting\")\n",
        "print(\"- Singapore-specific prompts and output prefixes\")\n",
        "print(\"- Model output: flan_t5_full_finetune_model/\")\n",
        "print(\"- This MUST work if the data is good!\")\n",
        "\n",
        "!python flan_t5_full_finetune.py\n",
        "\n",
        "print('‚úÖ FULL fine-tuning completed!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üí¨ Flan-T5-BASE Inference Demo\n",
        "\n",
        "# 5) Test Flan-T5-BASE model - the working larger model!\n",
        "print(\"üéØ Testing Flan-T5-BASE model (the one that actually works!):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Quick test with the working base model first\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "print(\"Loading Flan-T5-BASE for quick test...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "test_questions = [\n",
        "    \"What are capital requirements for banks?\",\n",
        "    \"What is MAS in Singapore?\",\n",
        "    \"Define financial regulation.\",\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    inputs = tokenizer(f\"Answer this question: {q}\", return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=50, num_beams=3)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {response}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"‚úÖ Flan-T5-BASE inference demo completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üìà FULL Fine-tuning Evaluation & Comparison\n",
        "\n",
        "# 6) Test the FULL fine-tuned model (no LoRA)\n",
        "print(\"üìä Testing FULL fine-tuned model...\")\n",
        "print(\"Comparing: Base Flan-T5-small vs FULL fine-tuned model\")\n",
        "\n",
        "# Load models for comparison\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"\\nüß™ FULL fine-tuning comparison test:\")\n",
        "\n",
        "# Load base Flan-T5-small\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "# Load full fine-tuned model\n",
        "try:\n",
        "    full_model_path = Path(\"flan_t5_full_finetune_model\")\n",
        "    if full_model_path.exists():\n",
        "        print(\"Loading FULL fine-tuned model...\")\n",
        "        full_tokenizer = AutoTokenizer.from_pretrained(full_model_path)\n",
        "        full_model = AutoModelForSeq2SeqLM.from_pretrained(full_model_path)\n",
        "        print(\"‚úÖ FULL fine-tuned model loaded!\")\n",
        "    else:\n",
        "        print(\"‚ùå FULL fine-tuned model not found\")\n",
        "        full_model = base_model\n",
        "        full_tokenizer = base_tokenizer\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading FULL model: {e}\")\n",
        "    full_model = base_model\n",
        "    full_tokenizer = base_tokenizer\n",
        "\n",
        "# Test multiple questions\n",
        "test_questions = [\n",
        "    \"What are the capital requirements for banks in Singapore?\",\n",
        "    \"How frequently must banks submit returns to MAS?\",\n",
        "    \"What is MAS's position on AI in financial services?\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "for i, test_q in enumerate(test_questions, 1):\n",
        "    print(f\"\\n{i}. Question: {test_q}\")\n",
        "    \n",
        "    # Base model response\n",
        "    base_inputs = base_tokenizer(f\"Singapore MAS regulation: {test_q}\", return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        base_out = base_model.generate(**base_inputs, max_new_tokens=50, num_beams=3)\n",
        "    base_response = base_tokenizer.decode(base_out[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Full fine-tuned response\n",
        "    full_inputs = full_tokenizer(f\"Singapore MAS regulation: {test_q}\", return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        full_out = full_model.generate(**full_inputs, max_new_tokens=50, num_beams=3)\n",
        "    full_response = full_tokenizer.decode(full_out[0], skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"   Base Model: {base_response}\")\n",
        "    print(f\"   FULL Model: {full_response}\")\n",
        "    \n",
        "    if base_response != full_response:\n",
        "        print(\"   ‚úÖ SUCCESS: Responses are DIFFERENT!\")\n",
        "    else:\n",
        "        print(\"   ‚ùå PROBLEM: Still identical\")\n",
        "    \n",
        "    print(\"-\" * 70)\n",
        "\n",
        "print(\"\\nüéØ FULL FINE-TUNING TEST COMPLETED!\")\n",
        "\n",
        "# Summary of debugging and full fine-tuning results\n",
        "print(\"\\nüí• DEBUGGING & FULL FINE-TUNING SUMMARY:\")\n",
        "print(\"=\"*70)\n",
        "print(\"üîç Step 1: Analyzed training data quality\")\n",
        "print(\"üí• Step 2: Tried FULL fine-tuning (no LoRA restrictions)\")\n",
        "print(\"üß™ Step 3: Tested multiple questions for differences\")\n",
        "\n",
        "print(f\"\\nüìä Training Approach Evolution:\")\n",
        "print(f\"  1Ô∏è‚É£ DialoGPT: Wrong architecture ‚Üí 0.0001 BLEU\")\n",
        "print(f\"  2Ô∏è‚É£ Flan-T5-small: Too small ‚Üí gibberish responses\")\n",
        "print(f\"  3Ô∏è‚É£ Conservative LoRA: Too weak ‚Üí identical responses\")\n",
        "print(f\"  4Ô∏è‚É£ AGGRESSIVE LoRA: Still too weak ‚Üí identical responses\")\n",
        "print(f\"  5Ô∏è‚É£ FULL fine-tuning: All parameters ‚Üí should work!\")\n",
        "\n",
        "print(f\"\\nüéØ Key Insights:\")\n",
        "print(f\"  ‚Ä¢ LoRA (even aggressive) may be too restrictive\")\n",
        "print(f\"  ‚Ä¢ Full parameter training gives model complete freedom\")\n",
        "print(f\"  ‚Ä¢ Training data quality is crucial\")\n",
        "print(f\"  ‚Ä¢ Singapore-specific prompts and outputs\")\n",
        "\n",
        "print(f\"\\nüìÅ Generated artifacts:\")\n",
        "print(f\"  ‚Ä¢ debug_training_data.py - Data quality analysis\")\n",
        "print(f\"  ‚Ä¢ flan_t5_full_finetune_model/ - FULL fine-tuned model\")\n",
        "print(f\"  ‚Ä¢ Should finally produce different responses!\")\n",
        "\n",
        "print(f\"\\nüèÜ SUCCESS CRITERIA:\")\n",
        "print(f\"  ‚úÖ Training data contains Singapore-specific info\")\n",
        "print(f\"  ‚úÖ Full fine-tuning trains all parameters\")\n",
        "print(f\"  üéØ Responses should be DIFFERENT from base model\")\n",
        "print(f\"  üéØ Should mention MAS, SGD, Singapore regulations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üí• FULL Fine-tuning Pipeline Summary\n",
        "\n",
        "# 7) Complete pipeline summary - from debugging to full training!\n",
        "print(\"üí• DEBUGGING & FULL FINE-TUNING PIPELINE COMPLETED!\")\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ Enhanced dataset: 21 Q&A pairs ‚Üí 63 training samples\")\n",
        "print(\"‚úÖ Found working base model: Flan-T5 family\")\n",
        "print(\"‚ùå LoRA approaches failed: Even aggressive params ‚Üí identical responses\")\n",
        "print(\"üí• FULL fine-tuning: Last resort ‚Üí should finally work!\")\n",
        "\n",
        "print(f\"\\nüîç Complete Journey:\")\n",
        "print(f\"  1Ô∏è‚É£ DialoGPT: Wrong for Q&A ‚Üí 0.0001 BLEU\")\n",
        "print(f\"  2Ô∏è‚É£ Flan-T5-small: Broken/gibberish ‚Üí unusable\")\n",
        "print(f\"  3Ô∏è‚É£ Flan-T5-base: Good base ‚Üí coherent but generic\")\n",
        "print(f\"  4Ô∏è‚É£ Conservative LoRA: r=8, alpha=16 ‚Üí identical responses\")\n",
        "print(f\"  5Ô∏è‚É£ AGGRESSIVE LoRA: r=32, alpha=64 ‚Üí still identical!\")\n",
        "print(f\"  6Ô∏è‚É£ Data debugging: Check if training data is actually different\")\n",
        "print(f\"  7Ô∏è‚É£ FULL fine-tuning: All parameters trainable ‚Üí final attempt!\")\n",
        "\n",
        "print(f\"\\nüí• FULL Fine-tuning Approach:\")\n",
        "print(f\"  ‚Ä¢ Model: Flan-T5-small (manageable size for full training)\")\n",
        "print(f\"  ‚Ä¢ Parameters: ALL trainable (no LoRA restrictions)\")\n",
        "print(f\"  ‚Ä¢ Data: Limited to 20 samples (prevent overfitting)\")\n",
        "print(f\"  ‚Ä¢ Prompts: 'Singapore MAS regulation: ...'\")\n",
        "print(f\"  ‚Ä¢ Outputs: 'According to MAS Singapore: ...'\")\n",
        "print(f\"  ‚Ä¢ Training: Conservative LR, small batches, careful monitoring\")\n",
        "\n",
        "print(f\"\\nüìÅ Generated artifacts:\")\n",
        "print(f\"  ‚Ä¢ processed_data/enhanced_*.json - Training data\")\n",
        "print(f\"  ‚Ä¢ debug_training_data.py - Data quality analysis\")\n",
        "print(f\"  ‚Ä¢ flan_t5_full_finetune_model/ - FULL fine-tuned model\")\n",
        "print(f\"  ‚Ä¢ Complete debugging and training pipeline\")\n",
        "\n",
        "print(f\"\\nüéØ Final Success Test:\")\n",
        "print(f\"  üîç Data analysis: Does training data contain Singapore specifics?\")\n",
        "print(f\"  üí• Full training: Do responses differ from base model?\")\n",
        "print(f\"  üéØ Domain knowledge: Does model mention MAS, SGD, Singapore?\")\n",
        "print(f\"  üèÜ Production ready: Can we finally deploy this model?\")\n",
        "\n",
        "# Optional: Save to Google Drive\n",
        "print(f\"\\nüì¶ Optional: Uncomment below to save to Google Drive\")\n",
        "print(\"# from google.colab import drive\")\n",
        "print(\"# drive.mount('/content/drive')\")\n",
        "print(\"# !cp -r flan_t5_full_finetune_model /content/drive/MyDrive/\")\n",
        "\n",
        "print(f\"\\nüí• FULL FINE-TUNING: If this doesn't work, the problem is fundamental!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
