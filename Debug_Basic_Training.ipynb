{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üö® SUPER SIMPLE Debug - Test Basic Concepts\n",
        "\n",
        "**After complete failure, let's test the absolute fundamentals:**\n",
        "\n",
        "## üîç What We're Testing:\n",
        "1. **Different models** - Find one that gives sensible responses\n",
        "2. **Manual weight changes** - Can we affect output at all?\n",
        "3. **Tiny training** - Does 1 example + 1 epoch change anything?\n",
        "\n",
        "## üéØ Goal:\n",
        "Identify if the problem is:\n",
        "- **Model choice** (wrong/broken model)\n",
        "- **Training setup** (LoRA not working) \n",
        "- **Fundamental misunderstanding**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Test 1: Different Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "print(\"üîç TESTING DIFFERENT MODELS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "models_to_test = [\n",
        "    \"google/flan-t5-small\",\n",
        "    \"google/flan-t5-base\", \n",
        "    \"t5-small\",\n",
        "    \"google/t5-efficient-tiny\"\n",
        "]\n",
        "\n",
        "test_question = \"What currency does Singapore use?\"\n",
        "\n",
        "for model_name in models_to_test:\n",
        "    print(f\"\\nüìä Testing: {model_name}\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "        \n",
        "        inputs = tokenizer(test_question, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=20, num_beams=2)\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        print(f\"   Response: '{response}'\")\n",
        "        \n",
        "        # Check if response makes sense\n",
        "        if \"singapore\" in response.lower() or \"sgd\" in response.lower() or \"dollar\" in response.lower():\n",
        "            print(\"   ‚úÖ SENSIBLE response!\")\n",
        "        else:\n",
        "            print(\"   ‚ùå Nonsense response\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Failed to load: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Test 2: Manual Weight Changes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß TESTING MANUAL WEIGHT CHANGES\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Load model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "    \n",
        "    test_input = \"What is Singapore?\"\n",
        "    inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
        "    \n",
        "    # Get original response\n",
        "    with torch.no_grad():\n",
        "        original_outputs = model.generate(**inputs, max_new_tokens=10, num_beams=2)\n",
        "    original_response = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Original response: '{original_response}'\")\n",
        "    \n",
        "    # Manually modify a weight (just to see if anything changes)\n",
        "    print(\"\\nManually changing model weights...\")\n",
        "    with torch.no_grad():\n",
        "        # Find first parameter and add some noise\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad and len(param.shape) > 1:\n",
        "                print(f\"Modifying: {name}\")\n",
        "                param.data += torch.randn_like(param.data) * 0.01  # Small random noise\n",
        "                break\n",
        "    \n",
        "    # Get new response\n",
        "    with torch.no_grad():\n",
        "        new_outputs = model.generate(**inputs, max_new_tokens=10, num_beams=2)\n",
        "    new_response = tokenizer.decode(new_outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Modified response: '{new_response}'\")\n",
        "    \n",
        "    if original_response != new_response:\n",
        "        print(\"‚úÖ SUCCESS: Manual weight change affected output!\")\n",
        "        manual_works = True\n",
        "    else:\n",
        "        print(\"‚ùå PROBLEM: Manual weight change had no effect\")\n",
        "        manual_works = False\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Manual weight test failed: {e}\")\n",
        "    manual_works = False\n",
        "\n",
        "print(f\"\\nüéØ Manual weight changes work: {manual_works}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
