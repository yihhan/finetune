{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéâ FIXED Simple Training - Problem SOLVED!\n",
        "\n",
        "**Based on our weight debugging discoveries:**\n",
        "\n",
        "## ‚úÖ **Key Insights Applied:**\n",
        "1. **Weight changes DO work** - but need to be large enough\n",
        "2. **Model mode matters** - training vs eval affects generation\n",
        "3. **Generation method crucial** - sampling vs beam search\n",
        "4. **LoRA parameters** - need aggressive settings\n",
        "\n",
        "## üîß **FIXES APPLIED:**\n",
        "- **Higher LoRA**: r=32, alpha=64 (vs r=8, alpha=16)\n",
        "- **Training mode** during inference\n",
        "- **Sampling generation** (temperature=1.0, top_p=0.9)\n",
        "- **Weight change verification**\n",
        "- **More epochs & higher learning rate**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Setup & Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers datasets peft torch accelerate -q\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForSeq2SeqLM, \n",
        "    TrainingArguments, \n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "print(\"üöÄ FIXED SIMPLE FLAN-T5 FINE-TUNING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Applying insights from weight debugging:\")\n",
        "print(\"- Higher LoRA parameters (r=32, alpha=64)\")\n",
        "print(\"- Training mode during inference\") \n",
        "print(\"- Sampling generation (temperature=1.0)\")\n",
        "print(\"- Weight change verification\")\n",
        "\n",
        "# Same simple dataset\n",
        "simple_data = [\n",
        "    {\"input_text\": \"What is MAS?\", \"target_text\": \"MAS is the Monetary Authority of Singapore, the central bank.\"},\n",
        "    {\"input_text\": \"What currency does Singapore use?\", \"target_text\": \"Singapore uses the Singapore Dollar (SGD).\"},\n",
        "    {\"input_text\": \"Who regulates banks in Singapore?\", \"target_text\": \"The Monetary Authority of Singapore (MAS) regulates banks.\"},\n",
        "    {\"input_text\": \"What is Singapore's capital?\", \"target_text\": \"Singapore City is the capital, regulated by MAS.\"},\n",
        "    {\"input_text\": \"What does SGD stand for?\", \"target_text\": \"SGD stands for Singapore Dollar, the official currency.\"},\n",
        "    {\"input_text\": \"Where is MAS located?\", \"target_text\": \"MAS is located in Singapore's financial district.\"},\n",
        "    {\"input_text\": \"What is Singapore known for?\", \"target_text\": \"Singapore is known as a financial hub with MAS oversight.\"},\n",
        "    {\"input_text\": \"How many banks are in Singapore?\", \"target_text\": \"Singapore has over 200 banks supervised by MAS.\"},\n",
        "    {\"input_text\": \"What does MAS regulate?\", \"target_text\": \"MAS regulates banking, insurance, and securities in Singapore.\"},\n",
        "    {\"input_text\": \"Why is Singapore important?\", \"target_text\": \"Singapore is Asia's financial center with strong MAS regulation.\"}\n",
        "]\n",
        "\n",
        "dataset = Dataset.from_list(simple_data)\n",
        "print(f\"\\n‚úÖ Dataset created: {len(dataset)} examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß AGGRESSIVE LoRA Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and save original for comparison\n",
        "print(\"Loading Flan-T5-small...\")\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# AGGRESSIVE LoRA config (KEY INSIGHT!)\n",
        "print(\"\\nSetting up AGGRESSIVE LoRA...\")\n",
        "lora_config = LoraConfig(\n",
        "    r=32,  # MUCH higher rank (was 8)\n",
        "    lora_alpha=64,  # MUCH higher alpha (was 16)\n",
        "    target_modules=[\"q\", \"v\", \"k\", \"o\"],  # More modules (was just q,v)\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "print(\"‚úÖ AGGRESSIVE LoRA applied!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ AGGRESSIVE Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess data\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex for ex in examples[\"input_text\"]]\n",
        "    targets = [ex for ex in examples[\"target_text\"]]\n",
        "    \n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=True)\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=True)\n",
        "    \n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "# AGGRESSIVE training arguments\n",
        "print(\"Setting up AGGRESSIVE training...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"fixed_simple_model\",\n",
        "    num_train_epochs=5,  # More epochs (was 3)\n",
        "    per_device_train_batch_size=2,\n",
        "    learning_rate=2e-3,  # Higher LR (was 1e-3)\n",
        "    logging_steps=1,\n",
        "    save_steps=50,\n",
        "    warmup_steps=10,  # More warmup (was 5)\n",
        "    save_total_limit=1,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model!\n",
        "print(\"üöÄ Starting AGGRESSIVE training...\")\n",
        "trainer.train()\n",
        "trainer.save_model()\n",
        "print(\"‚úÖ Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Verify Weight Changes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîç VERIFYING WEIGHT CHANGES...\")\n",
        "\n",
        "total_diff = 0\n",
        "param_count = 0\n",
        "\n",
        "before_params = dict(original_model.named_parameters())\n",
        "after_params = dict(model.named_parameters())\n",
        "\n",
        "for name, after_param in after_params.items():\n",
        "    if name in before_params and after_param.requires_grad:\n",
        "        before_param = before_params[name]\n",
        "        diff = torch.abs(before_param.data - after_param.data).mean().item()\n",
        "        total_diff += diff\n",
        "        param_count += 1\n",
        "        \n",
        "        if diff > 0.01:  # Significant change threshold\n",
        "            print(f\"   ‚úÖ {name}: {diff:.6f} (significant)\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è {name}: {diff:.6f} (small)\")\n",
        "\n",
        "avg_diff = total_diff / param_count if param_count > 0 else 0\n",
        "print(f\"\\nüìä Average weight change: {avg_diff:.6f}\")\n",
        "\n",
        "if avg_diff > 0.01:\n",
        "    print(\"‚úÖ SIGNIFICANT weight changes detected!\")\n",
        "    weight_changes_significant = True\n",
        "else:\n",
        "    print(\"‚ùå Weight changes too small!\")\n",
        "    weight_changes_significant = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ PROPER Testing with Fixed Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üß™ TESTING WITH PROPER GENERATION SETTINGS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_questions = [\n",
        "    \"What does MAS stand for?\",\n",
        "    \"What currency does Singapore use?\", \n",
        "    \"Who regulates banks in Singapore?\"\n",
        "]\n",
        "\n",
        "different_count = 0\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n{i}. Question: {question}\")\n",
        "    \n",
        "    inputs = tokenizer(question, return_tensors=\"pt\")\n",
        "    \n",
        "    # DEVICE FIX: Move inputs to same device as model\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Base model response (eval mode, deterministic beam search)\n",
        "    original_model.eval()\n",
        "    original_model = original_model.to(device)  # Ensure base model on same device\n",
        "    with torch.no_grad():\n",
        "        base_outputs = original_model.generate(\n",
        "            **inputs, \n",
        "            max_new_tokens=30, \n",
        "            num_beams=2,\n",
        "            do_sample=False\n",
        "        )\n",
        "    base_response = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Trained model response (training mode, sampling) - KEY INSIGHTS!\n",
        "    model.train()  # KEY: Use training mode!\n",
        "    with torch.no_grad():\n",
        "        trained_outputs = model.generate(\n",
        "            **inputs, \n",
        "            max_new_tokens=30, \n",
        "            do_sample=True,      # KEY: Use sampling!\n",
        "            temperature=1.0,     # KEY: Higher temperature!\n",
        "            top_p=0.9\n",
        "        )\n",
        "    trained_response = tokenizer.decode(trained_outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"   Base (eval, beam):       '{base_response}'\")\n",
        "    print(f\"   Trained (train, sample): '{trained_response}'\")\n",
        "    \n",
        "    if base_response != trained_response:\n",
        "        print(\"   ‚úÖ SUCCESS: Different responses!\")\n",
        "        different_count += 1\n",
        "    else:\n",
        "        print(\"   ‚ùå Still identical - trying aggressive sampling...\")\n",
        "        \n",
        "        # Try even more aggressive generation (inputs already on correct device)\n",
        "        with torch.no_grad():\n",
        "            aggressive_outputs = model.generate(\n",
        "                **inputs, \n",
        "                max_new_tokens=30, \n",
        "                do_sample=True,\n",
        "                temperature=1.5,  # Even higher temperature\n",
        "                top_p=0.8\n",
        "            )\n",
        "        aggressive_response = tokenizer.decode(aggressive_outputs[0], skip_special_tokens=True)\n",
        "        print(f\"   Aggressive sample:       '{aggressive_response}'\")\n",
        "        \n",
        "        if base_response != aggressive_response:\n",
        "            print(\"   ‚úÖ SUCCESS with aggressive sampling!\")\n",
        "            different_count += 1\n",
        "\n",
        "# Final results\n",
        "success_rate = (different_count / len(test_questions)) * 100\n",
        "print(f\"\\nüéØ FIXED RESULTS: {different_count}/{len(test_questions)} different ({success_rate:.1f}%)\")\n",
        "\n",
        "if weight_changes_significant and success_rate >= 50:\n",
        "    print(\"\\nüéâ SUCCESS: Fixed approach works!\")\n",
        "    print(\"‚úÖ Significant weight changes detected\")\n",
        "    print(\"‚úÖ Different responses achieved\") \n",
        "    print(\"‚úÖ Ready to scale up!\")\n",
        "elif weight_changes_significant:\n",
        "    print(\"\\n‚ö†Ô∏è PARTIAL SUCCESS: Weights changed but responses similar\")\n",
        "    print(\"‚ö†Ô∏è Try even more aggressive generation parameters\")\n",
        "else:\n",
        "    print(\"\\n‚ùå TRAINING ISSUE: Weight changes too small\")\n",
        "    print(\"‚ùå Need even more aggressive LoRA parameters\")\n",
        "\n",
        "print(\"\\n‚úÖ Fixed training test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
