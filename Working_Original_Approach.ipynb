{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 WORKING Original Approach - From Successful Scripts\n",
        "\n",
        "## ✅ **Based on ACTUAL WORKING CODE:**\n",
        "- **train.py** - DialoGPT-medium with LoRA (r=16, alpha=32)\n",
        "- **inference.py** - Proper prompt format and generation parameters\n",
        "- **demo.py** - Complete pipeline that produces good results\n",
        "- **README.md** - Shows 7.25x-10.37x BLEU improvement achieved\n",
        "\n",
        "## 🔍 **This Uses the EXACT Code That Worked:**\n",
        "- **DialoGPT-medium** (not GPT-2)\n",
        "- **Proper instruction format**: \"### Instruction:\\nAnswer the following question about Singapore financial regulations:\\n\\n### Input:\\n{question}\\n\\n### Response:\\n\"\n",
        "- **Working generation parameters** from inference.py\n",
        "- **Proven LoRA config** from train.py\n",
        "\n",
        "## 🚀 **Expected Results: Professional Singapore Financial Responses**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🚀 SETUP - EXACT WORKING CONFIGURATION FROM SCRIPTS\n",
        "!pip install torch transformers datasets peft accelerate rouge-score nltk sentence-transformers -q\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, Any, List\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, \n",
        "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from datasets import Dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✅ Setup complete! Using device: {device}\")\n",
        "print(\"🎯 Using EXACT working approach from train.py, inference.py, demo.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🤖 EXACT WORKING MODEL SETUP (From train.py)\n",
        "print(\"🤖 Setting up DialoGPT-medium with EXACT working LoRA config...\")\n",
        "\n",
        "# EXACT model from train.py that produced good results\n",
        "model_name = \"microsoft/DialoGPT-medium\"  # NOT GPT-2!\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# EXACT LoRA config from train.py (LoRAArguments class)\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,                    # From train.py default\n",
        "    lora_alpha=32,          # From train.py default  \n",
        "    lora_dropout=0.1,       # From train.py default\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # From train.py default\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(f\"✅ DialoGPT-medium loaded on {device}\")\n",
        "print(\"🎯 Using EXACT LoRA config from working train.py!\")\n",
        "print(\"📊 This configuration achieved 7.25x-10.37x BLEU improvement\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📊 WORKING DATASET (From enhanced_training_data.json)\n",
        "print(\"📊 Loading the EXACT dataset that produced successful results...\")\n",
        "\n",
        "# Load the actual working dataset from enhanced_training_data.json\n",
        "with open(\"processed_data/enhanced_training_data.json\", \"r\") as f:\n",
        "    enhanced_data = json.load(f)\n",
        "\n",
        "print(f\"✅ Loaded {len(enhanced_data)} examples from enhanced_training_data.json\")\n",
        "\n",
        "# Convert to training format (first few examples)\n",
        "training_texts = []\n",
        "for item in enhanced_data[:20]:  # Use first 20 examples\n",
        "    # Use the exact format from the working dataset\n",
        "    text = f\"{item['instruction']}\\n\\n{item['input']}\\n\\n{item['output']}\"\n",
        "    training_texts.append({\"text\": text})\n",
        "\n",
        "print(f\"📝 Sample format:\")\n",
        "print(f\"   {training_texts[0]['text'][:200]}...\")\n",
        "print(f\"\\n🎯 This is the EXACT dataset format that produced professional responses!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📚 WORKING DATA PREPARATION (From train.py)\n",
        "print(\"📚 Preparing data with EXACT working tokenization...\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"EXACT tokenization from train.py DataTrainingArguments\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,  # From train.py default max_seq_length\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "# Create and tokenize dataset\n",
        "dataset = Dataset.from_list(training_texts)\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Data collator (same as train.py)\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "print(f\"✅ Tokenized {len(tokenized_dataset)} examples\")\n",
        "print(f\"📏 Max length: 512 tokens (from working train.py)\")\n",
        "print(f\"🎯 Using EXACT tokenization that produced professional responses\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🏋️ WORKING TRAINING PARAMETERS (From train.py)\n",
        "print(\"🏋️ Training with EXACT working parameters from train.py...\")\n",
        "\n",
        "# EXACT training arguments from train.py create_training_arguments function\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"working_finetuned_model\",\n",
        "    num_train_epochs=3,              # From train.py default\n",
        "    per_device_train_batch_size=4,   # From train.py default\n",
        "    per_device_eval_batch_size=4,    # From train.py default\n",
        "    learning_rate=5e-5,              # From train.py default\n",
        "    warmup_steps=100,                # From train.py default\n",
        "    logging_steps=10,                # From train.py default\n",
        "    save_steps=500,                  # From train.py default\n",
        "    save_total_limit=2,              # From train.py default\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None,\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train with EXACT working settings\n",
        "print(\"🚀 Training with EXACT parameters from working train.py...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save model (same structure as train.py)\n",
        "model.save_pretrained(\"working_finetuned_model\")\n",
        "tokenizer.save_pretrained(\"working_finetuned_model\")\n",
        "\n",
        "print(\"✅ Training completed with WORKING train.py parameters!\")\n",
        "print(\"💾 Model saved - should produce professional Singapore financial responses!\")\n",
        "print(\"🎯 Expected: 7.25x-10.37x BLEU improvement like README.md shows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🧪 WORKING INFERENCE TEST (From inference.py)\n",
        "print(\"🧪 Testing with EXACT inference approach from inference.py...\")\n",
        "\n",
        "def create_prompt_working(question: str) -> str:\n",
        "    \"\"\"EXACT prompt format from inference.py create_prompt method\"\"\"\n",
        "    prompt = f\"### Instruction:\\nAnswer the following question about Singapore financial regulations:\\n\\n### Input:\\n{question}\\n\\n### Response:\\n\"\n",
        "    return prompt\n",
        "\n",
        "def generate_response_working(model, question: str, max_length: int = 300) -> str:\n",
        "    \"\"\"EXACT generation from inference.py generate_response method\"\"\"\n",
        "    \n",
        "    # Create prompt (EXACT format from inference.py)\n",
        "    prompt = create_prompt_working(question)\n",
        "    \n",
        "    # Tokenize input (EXACT parameters from inference.py)\n",
        "    inputs = tokenizer(\n",
        "        prompt, \n",
        "        return_tensors=\"pt\", \n",
        "        truncation=True, \n",
        "        max_length=512  # From inference.py\n",
        "    )\n",
        "    \n",
        "    # Move to device\n",
        "    if device.type == \"cuda\":\n",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate response (EXACT parameters from inference.py)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,             # From inference.py default\n",
        "            top_p=0.9,                  # From inference.py default\n",
        "            do_sample=True,             # From inference.py default\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1,     # From inference.py\n",
        "        )\n",
        "    \n",
        "    # Decode response (EXACT logic from inference.py)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Remove the input prompt from response (EXACT logic from inference.py)\n",
        "    if prompt in response:\n",
        "        response = response.replace(prompt, \"\").strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Load base model for comparison\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\").to(device)\n",
        "\n",
        "# Test with questions from README.md examples\n",
        "test_questions = [\n",
        "    \"What is MAS's position on the use of artificial intelligence in financial advisory services?\",\n",
        "    \"What are the capital adequacy requirements for banks in Singapore?\",\n",
        "    \"What does MAS stand for?\",\n",
        "    \"What currency does Singapore use?\"\n",
        "]\n",
        "\n",
        "# Expected responses from README.md and example_results.json\n",
        "expected_responses = [\n",
        "    \"MAS supports the responsible use of AI in financial advisory services while ensuring adequate safeguards...\",\n",
        "    \"Singapore banks are required to maintain a minimum Common Equity Tier 1 (CET1) capital ratio of 6.5%...\",\n",
        "    \"MAS stands for Monetary Authority of Singapore...\",\n",
        "    \"Singapore uses the Singapore Dollar (SGD)...\"\n",
        "]\n",
        "\n",
        "print(\"\\n🎯 WORKING INFERENCE TEST (Using inference.py approach):\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "professional_count = 0\n",
        "total_tests = len(test_questions)\n",
        "\n",
        "for i, (question, expected) in enumerate(zip(test_questions, expected_responses), 1):\n",
        "    print(f\"\\n{i}. {question}\")\n",
        "    \n",
        "    base_response = generate_response_working(base_model, question)\n",
        "    ft_response = generate_response_working(model, question)\n",
        "    \n",
        "    print(f\"   Expected:   '{expected[:80]}...'\")\n",
        "    print(f\"   Base:       '{base_response[:80]}...'\")\n",
        "    print(f\"   Fine-tuned: '{ft_response[:80]}...'\")\n",
        "    \n",
        "    # Check for professional response (detailed and relevant)\n",
        "    is_professional = (\n",
        "        len(ft_response) > 30 and  # Substantial response\n",
        "        any(term in ft_response.lower() for term in ['singapore', 'mas', 'financial', 'capital', 'sgd']) and\n",
        "        not any(bad in ft_response.lower() for bad in ['program', 'hong kong', 'united states']) and\n",
        "        ft_response != base_response  # Different from base\n",
        "    )\n",
        "    \n",
        "    if is_professional:\n",
        "        print(f\"   ✅ PROFESSIONAL Singapore financial response!\")\n",
        "        professional_count += 1\n",
        "    else:\n",
        "        print(f\"   ❌ Poor quality or identical to base\")\n",
        "\n",
        "success_rate = professional_count / total_tests\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"🏆 PROFESSIONAL RESPONSE SUCCESS RATE: {success_rate:.1%}\")\n",
        "\n",
        "if success_rate >= 0.75:\n",
        "    print(f\"🎉 EXCELLENT: Matching the working approach success!\")\n",
        "    print(f\"🎯 Professional Singapore financial responses achieved!\")\n",
        "    print(f\"📊 Should match 7.25x-10.37x BLEU improvement from README.md\")\n",
        "elif success_rate >= 0.5:\n",
        "    print(f\"✅ GOOD: Significant improvement, but room for optimization\")\n",
        "else:\n",
        "    print(f\"❌ POOR: Still not matching the working approach\")\n",
        "\n",
        "print(f\"\\n💡 This uses the EXACT working code from train.py + inference.py\")\n",
        "print(f\"🎯 Expected: Professional responses like README.md examples!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
