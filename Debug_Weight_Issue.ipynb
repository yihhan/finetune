{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ BREAKTHROUGH: Debug Weight Issue\n",
        "\n",
        "**Critical Discovery:** Manual weight changes have **ZERO effect** on output!\n",
        "\n",
        "This explains why ALL training fails:\n",
        "- LoRA training = no effect\n",
        "- Full fine-tuning = no effect  \n",
        "- 496 samples = identical responses\n",
        "\n",
        "## üö® Root Cause:\n",
        "Weight modifications don't affect generation AT ALL. The generation pipeline is disconnected from the weights.\n",
        "\n",
        "## üîß Testing 5 Theories:\n",
        "1. **Generation too deterministic** \n",
        "2. **Weight changes not applied**\n",
        "3. **Model mode issues**\n",
        "4. **Massive changes test**\n",
        "5. **Forward pass vs generation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers torch -q\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load model once\n",
        "print(\"Loading Flan-T5-small...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "test_input = \"What is Singapore?\"\n",
        "inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
        "print(\"‚úÖ Model loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Theory 1: Generation Parameters Too Deterministic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üß™ THEORY 1: Generation parameters too deterministic\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Test different generation strategies\n",
        "generation_configs = [\n",
        "    {\"max_new_tokens\": 10, \"num_beams\": 1, \"do_sample\": False, \"name\": \"Greedy\"},\n",
        "    {\"max_new_tokens\": 10, \"num_beams\": 2, \"do_sample\": False, \"name\": \"Beam-2\"},\n",
        "    {\"max_new_tokens\": 10, \"do_sample\": True, \"temperature\": 0.7, \"name\": \"Sampling\"},\n",
        "    {\"max_new_tokens\": 10, \"do_sample\": True, \"temperature\": 1.5, \"name\": \"High-temp\"},\n",
        "]\n",
        "\n",
        "for config in generation_configs:\n",
        "    name = config.pop(\"name\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, **config)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"   {name}: '{response}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Theory 2: Weight Change Verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üß™ THEORY 2: Weight change verification\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Get original weight value\n",
        "target_param = None\n",
        "target_name = None\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad and len(param.shape) > 1:\n",
        "        target_param = param\n",
        "        target_name = name\n",
        "        break\n",
        "\n",
        "if target_param is not None:\n",
        "    original_weight = target_param.data.clone()\n",
        "    print(f\"Target parameter: {target_name}\")\n",
        "    print(f\"Original weight sample: {original_weight.flatten()[:5]}\")\n",
        "    \n",
        "    # Modify weight\n",
        "    target_param.data += torch.randn_like(target_param.data) * 0.1  # Larger change\n",
        "    modified_weight = target_param.data.clone()\n",
        "    print(f\"Modified weight sample: {modified_weight.flatten()[:5]}\")\n",
        "    \n",
        "    # Verify change happened\n",
        "    weight_diff = torch.abs(original_weight - modified_weight).mean()\n",
        "    print(f\"Weight difference: {weight_diff:.6f}\")\n",
        "    \n",
        "    if weight_diff > 0.001:\n",
        "        print(\"‚úÖ Weight change verified\")\n",
        "    else:\n",
        "        print(\"‚ùå Weight change too small\")\n",
        "else:\n",
        "    print(\"‚ùå No suitable parameter found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Theory 3: Model Mode Issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üß™ THEORY 3: Model mode and gradients\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(f\"Model training mode: {model.training}\")\n",
        "print(f\"Requires grad: {target_param.requires_grad if target_param is not None else 'N/A'}\")\n",
        "\n",
        "# Force training mode\n",
        "model.train()\n",
        "print(f\"After model.train(): {model.training}\")\n",
        "\n",
        "# Test generation in training mode\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=10, do_sample=True, temperature=1.0)\n",
        "response_train = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Training mode response: '{response_train}'\")\n",
        "\n",
        "# Back to eval mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=10, do_sample=True, temperature=1.0)\n",
        "response_eval = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Eval mode response: '{response_eval}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Theory 4: MASSIVE Weight Changes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üß™ THEORY 4: Massive weight change\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Make a HUGE change that should definitely affect output\n",
        "if target_param is not None:\n",
        "    # Get baseline response first\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=10, do_sample=True, temperature=1.0)\n",
        "    baseline_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Baseline response: '{baseline_response}'\")\n",
        "    \n",
        "    # Zero out the entire parameter\n",
        "    original_data = target_param.data.clone()\n",
        "    target_param.data.zero_()\n",
        "    \n",
        "    print(\"Zeroed out entire weight matrix...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=10, do_sample=True, temperature=1.0)\n",
        "    response_zero = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Zero weights response: '{response_zero}'\")\n",
        "    \n",
        "    # Restore original weights\n",
        "    target_param.data = original_data\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=10, do_sample=True, temperature=1.0)\n",
        "    response_restored = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Restored weights response: '{response_restored}'\")\n",
        "    \n",
        "    if response_zero != baseline_response:\n",
        "        print(\"‚úÖ MASSIVE change affected output!\")\n",
        "        massive_works = True\n",
        "    else:\n",
        "        print(\"‚ùå Even zeroing weights had no effect!\")\n",
        "        massive_works = False\n",
        "else:\n",
        "    print(\"‚ùå No parameter to test\")\n",
        "    massive_works = False\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
