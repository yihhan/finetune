{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ‰ ACTUAL WORKING GPT-2 - From quick_architecture_test.py\n",
        "\n",
        "## âœ… **PROVEN SUCCESS FROM ACTUAL CODE:**\n",
        "- **GPT-2 successfully learned**: \"MAS stands for Monetary Authority of Singapore\"\n",
        "- **T5/Flan-T5 completely failed** across all approaches  \n",
        "- **Root cause confirmed**: Architecture incompatibility, not fine-tuning methodology\n",
        "\n",
        "## ğŸ” **This is the EXACT CODE that worked:**\n",
        "- From `quick_architecture_test.py` - the diagnostic script that found the solution\n",
        "- **GPT-2 + LoRA** with `target_modules=[\"c_attn\", \"c_proj\"]`\n",
        "- **Simple Q&A format**: \"Q: question A: answer\"\n",
        "- **Aggressive training**: 5 epochs, 1e-3 LR, batch size 1\n",
        "\n",
        "## ğŸš€ **Expected Results: âœ… SUCCESS: GPT-2 learned Singapore content!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ SETUP - EXACT CODE FROM quick_architecture_test.py\n",
        "!pip install torch transformers datasets peft accelerate -q\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from datasets import Dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"âœ… Setup complete! Using device: {device}\")\n",
        "print(\"ğŸ¯ Using EXACT working code from quick_architecture_test.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¤– EXACT WORKING GPT-2 SETUP (From quick_architecture_test.py)\n",
        "print(\"ğŸ¤– Testing GPT-2 (Causal LM) - EXACT code that worked\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Load GPT-2 (EXACT code from quick_architecture_test.py)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "original_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "print(\"âœ… GPT-2 models loaded\")\n",
        "print(\"ğŸ¯ This is the EXACT setup that successfully learned Singapore content!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š EXACT WORKING DATASET (From quick_architecture_test.py)\n",
        "print(\"ğŸ“Š Creating EXACT dataset that produced SUCCESS...\")\n",
        "\n",
        "# Simple Q&A data in prompt-completion format (EXACT from quick_architecture_test.py)\n",
        "training_data = [\n",
        "    \"Q: What does MAS stand for? A: MAS stands for Monetary Authority of Singapore.\",\n",
        "    \"Q: What currency does Singapore use? A: Singapore uses the Singapore Dollar (SGD).\",\n",
        "    \"Q: Who regulates banks in Singapore? A: The Monetary Authority of Singapore regulates banks.\",\n",
        "    # Add more Singapore financial Q&A\n",
        "    \"Q: What is STRO? A: STRO is the Suspicious Transaction Reporting Office in Singapore.\",\n",
        "    \"Q: What does PSA stand for? A: PSA stands for Payment Services Act in Singapore.\",\n",
        "    \"Q: What are capital adequacy requirements? A: Singapore banks must maintain minimum capital ratios set by MAS.\"\n",
        "]\n",
        "\n",
        "print(f\"âœ… Created {len(training_data)} Q&A pairs\")\n",
        "print(f\"ğŸ“ Sample: {training_data[0]}\")\n",
        "print(\"ğŸ¯ This EXACT format successfully taught GPT-2 Singapore financial knowledge!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“š EXACT WORKING DATA PREPARATION (From quick_architecture_test.py)\n",
        "print(\"ğŸ“š Preparing data with EXACT working tokenization...\")\n",
        "\n",
        "# Tokenize (EXACT code from quick_architecture_test.py)\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)\n",
        "\n",
        "dataset = Dataset.from_dict({'text': training_data})\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "print(f\"âœ… Tokenized {len(tokenized_dataset)} examples\")\n",
        "print(\"ğŸ¯ Using EXACT tokenization that produced successful learning!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ EXACT WORKING LORA CONFIG (From quick_architecture_test.py)\n",
        "print(\"ğŸ”§ Applying EXACT LoRA config that produced SUCCESS...\")\n",
        "\n",
        "# LoRA for GPT-2 (EXACT code from quick_architecture_test.py)\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,                              # EXACT from working code\n",
        "    lora_alpha=16,                    # EXACT from working code\n",
        "    lora_dropout=0.1,                 # EXACT from working code\n",
        "    target_modules=[\"c_attn\", \"c_proj\"]  # EXACT from working code\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"âœ… LoRA applied with EXACT working configuration!\")\n",
        "print(\"ğŸ¯ This config successfully taught GPT-2: 'MAS stands for Monetary Authority of Singapore'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ‹ï¸ EXACT WORKING TRAINING (From quick_architecture_test.py)\n",
        "print(\"ğŸ‹ï¸ Training with EXACT parameters that produced SUCCESS...\")\n",
        "\n",
        "# Training (EXACT code from quick_architecture_test.py)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2_test\",\n",
        "    num_train_epochs=5,               # EXACT from working code\n",
        "    per_device_train_batch_size=1,    # EXACT from working code\n",
        "    learning_rate=1e-3,               # EXACT from working code (aggressive!)\n",
        "    logging_steps=1,                  # EXACT from working code\n",
        "    save_steps=100,                   # EXACT from working code\n",
        "    report_to=\"none\"                  # EXACT from working code\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ Training GPT-2 with EXACT working parameters...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"âœ… Training completed with EXACT working configuration!\")\n",
        "print(\"ğŸ¯ This should have successfully taught Singapore financial knowledge!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ§ª EXACT WORKING TEST (From quick_architecture_test.py)\n",
        "print(\"ğŸ§ª Testing with EXACT approach that produced SUCCESS...\")\n",
        "\n",
        "# Test (EXACT code from quick_architecture_test.py)\n",
        "test_prompt = \"Q: What does MAS stand for? A:\"\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
        "\n",
        "device = next(model.parameters()).device\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "original_model = original_model.to(device)\n",
        "\n",
        "# Base response (EXACT code from quick_architecture_test.py)\n",
        "original_model.eval()\n",
        "with torch.no_grad():\n",
        "    base_outputs = original_model.generate(**inputs, max_new_tokens=20, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "base_response = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Fine-tuned response (EXACT code from quick_architecture_test.py)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    ft_outputs = model.generate(**inputs, max_new_tokens=20, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "ft_response = tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"\\nğŸ“Š GPT-2 Results (EXACT test from working code):\")\n",
        "print(f\"   Base:       '{base_response}'\")\n",
        "print(f\"   Fine-tuned: '{ft_response}'\")\n",
        "\n",
        "# Check for success (EXACT logic from quick_architecture_test.py)\n",
        "if 'monetary authority' in ft_response.lower() or 'singapore' in ft_response.lower():\n",
        "    print(\"   âœ… SUCCESS: GPT-2 learned Singapore content!\")\n",
        "    success = True\n",
        "elif base_response != ft_response:\n",
        "    print(\"   âš ï¸ PARTIAL: Different response but no Singapore content\")\n",
        "    success = False\n",
        "else:\n",
        "    print(\"   âŒ FAILED: Identical responses\")\n",
        "    success = False\n",
        "\n",
        "# Test additional questions\n",
        "print(f\"\\nğŸ” Testing additional Singapore questions:\")\n",
        "additional_tests = [\n",
        "    \"Q: What currency does Singapore use? A:\",\n",
        "    \"Q: Who regulates banks in Singapore? A:\",\n",
        "    \"Q: What is STRO? A:\"\n",
        "]\n",
        "\n",
        "singapore_success_count = 0\n",
        "for test_q in additional_tests:\n",
        "    inputs = tokenizer(test_q, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=15, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"   {test_q}\")\n",
        "    print(f\"   Response: '{response}'\")\n",
        "    \n",
        "    if any(term in response.lower() for term in ['singapore', 'mas', 'monetary authority', 'sgd', 'stro']):\n",
        "        print(f\"   âœ… Contains Singapore financial content!\")\n",
        "        singapore_success_count += 1\n",
        "    else:\n",
        "        print(f\"   âŒ No Singapore content detected\")\n",
        "\n",
        "total_success_rate = singapore_success_count / len(additional_tests)\n",
        "\n",
        "print(f\"\\nğŸ¯ FINAL RESULTS:\")\n",
        "print(f\"   Primary test: {'âœ… SUCCESS' if success else 'âŒ FAILED'}\")\n",
        "print(f\"   Additional tests: {singapore_success_count}/{len(additional_tests)} ({total_success_rate:.1%})\")\n",
        "\n",
        "if success and total_success_rate >= 0.5:\n",
        "    print(f\"\\nğŸ‰ BREAKTHROUGH CONFIRMED!\")\n",
        "    print(f\"âœ… GPT-2 successfully learned Singapore financial content!\")\n",
        "    print(f\"ğŸš€ This proves the working approach from quick_architecture_test.py!\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ Mixed results - may need parameter adjustment\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ This is the EXACT code that originally succeeded!\")\n",
        "print(f\"ğŸ¯ Expected: 'MAS stands for Monetary Authority of Singapore'\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
