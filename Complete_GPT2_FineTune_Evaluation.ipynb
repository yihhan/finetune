{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 Complete GPT-2 Singapore Financial Regulation Fine-Tuning + Evaluation\n",
        "\n",
        "## 📋 **Complete Pipeline:**\n",
        "1. **Setup & Dependencies** - Install all required packages\n",
        "2. **Dataset Creation** - Generate Singapore financial Q&A dataset\n",
        "3. **Model Fine-Tuning** - Train GPT-2 with LoRA on Singapore data\n",
        "4. **Comprehensive Evaluation** - Multi-metric evaluation system\n",
        "5. **Results Analysis** - Performance assessment and comparison\n",
        "\n",
        "## ✅ **Expected Results:**\n",
        "- **High-quality Singapore financial responses**\n",
        "- **Significant improvement over base GPT-2**\n",
        "- **Production-ready evaluation metrics**\n",
        "- **Cost-effective alternative to GPT-4 RAG**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🚀 STEP 1: SETUP & DEPENDENCIES\n",
        "print(\"🚀 Installing dependencies for complete pipeline...\")\n",
        "\n",
        "!pip install torch transformers datasets peft accelerate -q\n",
        "!pip install rouge-score nltk sentence-transformers -q\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# Core ML libraries\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, \n",
        "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
        "from datasets import Dataset\n",
        "\n",
        "# Evaluation libraries\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "print(\"✅ All dependencies installed successfully!\")\n",
        "print(f\"🔥 Using device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📊 STEP 2: DATASET CREATION - SINGAPORE FINANCIAL Q&A\n",
        "print(\"📊 Creating comprehensive Singapore financial dataset...\")\n",
        "\n",
        "def create_singapore_financial_dataset():\n",
        "    \"\"\"Create comprehensive Singapore financial regulation Q&A dataset\"\"\"\n",
        "    \n",
        "    base_qa_pairs = [\n",
        "        {\n",
        "            \"question\": \"What does MAS stand for?\",\n",
        "            \"answer\": \"MAS stands for Monetary Authority of Singapore, which is Singapore's central bank and integrated financial regulator.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What currency does Singapore use?\",\n",
        "            \"answer\": \"Singapore uses the Singapore Dollar (SGD) as its official currency.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Who regulates banks in Singapore?\",\n",
        "            \"answer\": \"The Monetary Authority of Singapore (MAS) regulates banks in Singapore.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What are the minimum capital requirements for banks in Singapore?\",\n",
        "            \"answer\": \"Banks in Singapore must maintain a minimum Common Equity Tier 1 (CET1) capital ratio of 6.5% and a Total Capital Ratio of 10% as required by MAS.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"How often must banks report capital adequacy to MAS?\",\n",
        "            \"answer\": \"Banks must submit capital adequacy returns to MAS on a monthly basis.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is STRO and what does it do?\",\n",
        "            \"answer\": \"STRO is the Suspicious Transaction Reporting Office, which receives and analyzes suspicious transaction reports from financial institutions in Singapore.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What are the AML reporting requirements for financial institutions?\",\n",
        "            \"answer\": \"Financial institutions must report suspicious transactions to STRO within 15 days, regardless of the transaction amount.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is the minimum capital requirement for major payment institutions?\",\n",
        "            \"answer\": \"Major payment institutions must maintain minimum base capital of SGD 1 million under the Payment Services Act.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"How often must banks conduct penetration testing?\",\n",
        "            \"answer\": \"Banks must conduct penetration testing of critical systems at least annually as required by MAS Technology Risk Management Guidelines.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What are the cyber incident reporting requirements?\",\n",
        "            \"answer\": \"Financial institutions must report significant cyber incidents to MAS within 1 hour of discovery.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What does PDPA stand for and how does it apply to banks?\",\n",
        "            \"answer\": \"PDPA stands for Personal Data Protection Act. Banks must comply with PDPA requirements including obtaining consent for data collection and notifying individuals of data breaches within 72 hours.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is the minimum capital requirement for digital banks?\",\n",
        "            \"answer\": \"Digital banks must meet minimum paid-up capital of SGD 1.5 billion to obtain a banking license from MAS.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is the minimum Capital Adequacy Ratio for insurers?\",\n",
        "            \"answer\": \"Insurers in Singapore must maintain a minimum Capital Adequacy Ratio (CAR) of 120% under MAS's Risk-Based Capital framework.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What does SFA stand for in Singapore?\",\n",
        "            \"answer\": \"SFA stands for Securities and Futures Act, which governs Singapore's capital markets and requires licensing for securities activities.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What does PSA stand for in Singapore financial regulation?\",\n",
        "            \"answer\": \"PSA stands for Payment Services Act, which is Singapore's regulatory framework for payment services.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What are the licensing requirements for robo-advisory services?\",\n",
        "            \"answer\": \"Providers of robo-advisory services must hold a Capital Markets Services License for fund management and comply with MAS guidelines on algorithmic trading.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is the cooling-off period for investment products?\",\n",
        "            \"answer\": \"Customers have a 7-day cooling-off period for investment products purchased through digital channels, allowing them to cancel without penalty.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What are the operational resilience requirements?\",\n",
        "            \"answer\": \"Financial institutions must ensure critical business functions can resume within 2 hours of disruption and maintain business continuity plans.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is the maximum transaction threshold for enhanced due diligence?\",\n",
        "            \"answer\": \"Enhanced customer due diligence is required for transactions exceeding SGD 20,000 or equivalent in foreign currency.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What are the data breach notification requirements?\",\n",
        "            \"answer\": \"Financial institutions must notify MAS of data breaches within 1 hour of discovery if the breach involves customer information or affects operations.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is MAS's position on AI in financial services?\",\n",
        "            \"answer\": \"MAS supports responsible AI adoption in financial services while requiring institutions to ensure fairness, transparency, and accountability in AI systems.\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    # Create training format: \"Q: question A: answer\"\n",
        "    training_data = []\n",
        "    for qa in base_qa_pairs:\n",
        "        formatted_text = f\"Q: {qa['question']} A: {qa['answer']}\"\n",
        "        training_data.append({\"text\": formatted_text})\n",
        "    \n",
        "    return training_data, base_qa_pairs\n",
        "\n",
        "# Generate dataset\n",
        "training_data, qa_pairs = create_singapore_financial_dataset()\n",
        "\n",
        "print(f\"✅ Created dataset: {len(training_data)} training examples\")\n",
        "print(f\"📝 Sample: {training_data[0]['text'][:100]}...\")\n",
        "\n",
        "# Save dataset\n",
        "Path(\"data\").mkdir(exist_ok=True)\n",
        "with open(\"data/singapore_financial_qa.json\", \"w\") as f:\n",
        "    json.dump(training_data, f, indent=2)\n",
        "\n",
        "print(\"💾 Dataset saved to data/singapore_financial_qa.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🤖 STEP 3: MODEL SETUP & FINE-TUNING\n",
        "print(\"🤖 Setting up GPT-2 model for fine-tuning...\")\n",
        "\n",
        "# Model configuration\n",
        "model_name = \"gpt2\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# LoRA Configuration (optimized for GPT-2)\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,                    # Rank\n",
        "    lora_alpha=32,          # Alpha scaling\n",
        "    lora_dropout=0.05,      # Dropout\n",
        "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],  # All linear layers\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(f\"✅ Model loaded on {device}\")\n",
        "print(\"🔧 LoRA configuration applied successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📚 STEP 4: DATA PREPARATION & TOKENIZATION\n",
        "print(\"📚 Preparing training data...\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the training data\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_list(training_data)\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal LM, not masked LM\n",
        ")\n",
        "\n",
        "print(f\"✅ Tokenized {len(tokenized_dataset)} examples\")\n",
        "print(f\"📏 Max sequence length: 256 tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🏋️ STEP 5: TRAINING CONFIGURATION & EXECUTION\n",
        "print(\"🏋️ Starting fine-tuning...\")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_singapore_finetuned\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=50,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None,  # Disable wandb\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"🚀 Training started...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./gpt2_singapore_finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2_singapore_finetuned\")\n",
        "\n",
        "print(\"✅ Fine-tuning completed!\")\n",
        "print(\"💾 Model saved to ./gpt2_singapore_finetuned\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🧪 STEP 6: EVALUATION SETUP\n",
        "print(\"🧪 Setting up comprehensive evaluation system...\")\n",
        "\n",
        "# Load base and fine-tuned models for comparison\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "finetuned_model = PeftModel.from_pretrained(\n",
        "    AutoModelForCausalLM.from_pretrained(\"gpt2\"),\n",
        "    \"./gpt2_singapore_finetuned\"\n",
        ").to(device)\n",
        "\n",
        "# Initialize evaluation tools\n",
        "rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "smoothing = SmoothingFunction().method1\n",
        "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def generate_response(model, question, max_tokens=50):\n",
        "    \"\"\"Generate response and measure time\"\"\"\n",
        "    prompt = f\"Q: {question} A:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    end_time = time.time()\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \" A:\" in response:\n",
        "        response = response.split(\" A:\", 1)[1].strip()\n",
        "    \n",
        "    return response, end_time - start_time\n",
        "\n",
        "def calculate_metrics(reference, candidate):\n",
        "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
        "    # BLEU Score\n",
        "    try:\n",
        "        ref_tokens = reference.lower().split()\n",
        "        cand_tokens = candidate.lower().split()\n",
        "        bleu = sentence_bleu([ref_tokens], cand_tokens, smoothing_function=smoothing) if cand_tokens else 0.0\n",
        "    except:\n",
        "        bleu = 0.0\n",
        "    \n",
        "    # ROUGE Scores\n",
        "    try:\n",
        "        rouge_scores = rouge_scorer_obj.score(reference, candidate)\n",
        "        rouge_1 = rouge_scores['rouge1'].fmeasure\n",
        "        rouge_2 = rouge_scores['rouge2'].fmeasure\n",
        "        rouge_l = rouge_scores['rougeL'].fmeasure\n",
        "    except:\n",
        "        rouge_1 = rouge_2 = rouge_l = 0.0\n",
        "    \n",
        "    # Semantic Similarity\n",
        "    try:\n",
        "        embeddings = semantic_model.encode([reference, candidate])\n",
        "        similarity = np.dot(embeddings[0], embeddings[1]) / (\n",
        "            np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])\n",
        "        )\n",
        "    except:\n",
        "        similarity = 0.0\n",
        "    \n",
        "    # Singapore Domain Keywords\n",
        "    singapore_keywords = [\n",
        "        'mas', 'monetary authority', 'singapore', 'sgd', 'singapore dollar',\n",
        "        'stro', 'pdpa', 'psa', 'sfa', 'capital ratio', 'cet1', 'aml'\n",
        "    ]\n",
        "    \n",
        "    candidate_lower = candidate.lower()\n",
        "    singapore_matches = sum(1 for keyword in singapore_keywords if keyword in candidate_lower)\n",
        "    domain_accuracy = singapore_matches / len(singapore_keywords)\n",
        "    singapore_content = singapore_matches > 0\n",
        "    \n",
        "    return {\n",
        "        'bleu': bleu,\n",
        "        'rouge_1': rouge_1,\n",
        "        'rouge_2': rouge_2,\n",
        "        'rouge_l': rouge_l,\n",
        "        'semantic_similarity': float(similarity),\n",
        "        'domain_accuracy': domain_accuracy,\n",
        "        'singapore_content': singapore_content\n",
        "    }\n",
        "\n",
        "print(\"✅ Evaluation system ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📊 STEP 7: COMPREHENSIVE EVALUATION\n",
        "print(\"📊 Running comprehensive evaluation...\")\n",
        "\n",
        "# Test questions with ground truth\n",
        "test_questions = [\n",
        "    {\n",
        "        \"question\": \"What does MAS stand for?\",\n",
        "        \"ground_truth\": \"MAS stands for Monetary Authority of Singapore, which is Singapore's central bank and integrated financial regulator.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What currency does Singapore use?\",\n",
        "        \"ground_truth\": \"Singapore uses the Singapore Dollar (SGD) as its official currency.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Who regulates banks in Singapore?\",\n",
        "        \"ground_truth\": \"The Monetary Authority of Singapore (MAS) regulates banks in Singapore.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the minimum capital requirements for banks in Singapore?\",\n",
        "        \"ground_truth\": \"Banks in Singapore must maintain a minimum Common Equity Tier 1 (CET1) capital ratio of 6.5% and a Total Capital Ratio of 10% as required by MAS.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How often must banks report capital adequacy to MAS?\",\n",
        "        \"ground_truth\": \"Banks must submit capital adequacy returns to MAS on a monthly basis.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is STRO and what does it do?\",\n",
        "        \"ground_truth\": \"STRO is the Suspicious Transaction Reporting Office, which receives and analyzes suspicious transaction reports from financial institutions in Singapore.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the AML reporting requirements for financial institutions?\",\n",
        "        \"ground_truth\": \"Financial institutions must report suspicious transactions to STRO within 15 days, regardless of the transaction amount.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the minimum capital requirement for major payment institutions?\",\n",
        "        \"ground_truth\": \"Major payment institutions must maintain minimum base capital of SGD 1 million under the Payment Services Act.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Run evaluation\n",
        "print(\"\\n🧪 COMPREHENSIVE EVALUATION RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "all_results = []\n",
        "total_metrics = {\n",
        "    'base_bleu': [], 'ft_bleu': [],\n",
        "    'base_rouge_l': [], 'ft_rouge_l': [],\n",
        "    'base_semantic': [], 'ft_semantic': [],\n",
        "    'base_domain': [], 'ft_domain': [],\n",
        "    'base_singapore': [], 'ft_singapore': [],\n",
        "    'base_time': [], 'ft_time': []\n",
        "}\n",
        "\n",
        "for i, item in enumerate(test_questions, 1):\n",
        "    question = item['question']\n",
        "    ground_truth = item['ground_truth']\n",
        "    \n",
        "    print(f\"\\n{i}/{len(test_questions)}: {question}\")\n",
        "    \n",
        "    # Generate responses\n",
        "    base_response, base_time = generate_response(base_model, question)\n",
        "    ft_response, ft_time = generate_response(finetuned_model, question)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    base_metrics = calculate_metrics(ground_truth, base_response)\n",
        "    ft_metrics = calculate_metrics(ground_truth, ft_response)\n",
        "    \n",
        "    # Store results\n",
        "    total_metrics['base_bleu'].append(base_metrics['bleu'])\n",
        "    total_metrics['ft_bleu'].append(ft_metrics['bleu'])\n",
        "    total_metrics['base_rouge_l'].append(base_metrics['rouge_l'])\n",
        "    total_metrics['ft_rouge_l'].append(ft_metrics['rouge_l'])\n",
        "    total_metrics['base_semantic'].append(base_metrics['semantic_similarity'])\n",
        "    total_metrics['ft_semantic'].append(ft_metrics['semantic_similarity'])\n",
        "    total_metrics['base_domain'].append(base_metrics['domain_accuracy'])\n",
        "    total_metrics['ft_domain'].append(ft_metrics['domain_accuracy'])\n",
        "    total_metrics['base_singapore'].append(base_metrics['singapore_content'])\n",
        "    total_metrics['ft_singapore'].append(ft_metrics['singapore_content'])\n",
        "    total_metrics['base_time'].append(base_time)\n",
        "    total_metrics['ft_time'].append(ft_time)\n",
        "    \n",
        "    # Display comparison\n",
        "    print(f\"   📊 BLEU: Base={base_metrics['bleu']:.3f} | Fine-tuned={ft_metrics['bleu']:.3f} | Improvement={ft_metrics['bleu']/max(base_metrics['bleu'], 0.001):.1f}x\")\n",
        "    print(f\"   📊 Domain: Base={base_metrics['domain_accuracy']:.3f} | Fine-tuned={ft_metrics['domain_accuracy']:.3f}\")\n",
        "    print(f\"   🔍 Base:       '{base_response[:80]}{'...' if len(base_response) > 80 else ''}'\")\n",
        "    print(f\"   🎯 Fine-tuned: '{ft_response[:80]}{'...' if len(ft_response) > 80 else ''}'\")\n",
        "    \n",
        "    # Check for Singapore content\n",
        "    if ft_metrics['singapore_content']:\n",
        "        print(f\"   ✅ Contains Singapore financial content\")\n",
        "    else:\n",
        "        print(f\"   ❌ Missing Singapore financial content\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(\"🎯 FINAL EVALUATION SUMMARY\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📈 STEP 8: RESULTS ANALYSIS & SUMMARY\n",
        "print(\"📈 Analyzing results...\")\n",
        "\n",
        "# Calculate aggregate metrics\n",
        "results_summary = {\n",
        "    'base_model': {\n",
        "        'avg_bleu': np.mean(total_metrics['base_bleu']),\n",
        "        'avg_rouge_l': np.mean(total_metrics['base_rouge_l']),\n",
        "        'avg_semantic': np.mean(total_metrics['base_semantic']),\n",
        "        'avg_domain': np.mean(total_metrics['base_domain']),\n",
        "        'singapore_content_rate': np.mean(total_metrics['base_singapore']),\n",
        "        'avg_response_time': np.mean(total_metrics['base_time'])\n",
        "    },\n",
        "    'finetuned_model': {\n",
        "        'avg_bleu': np.mean(total_metrics['ft_bleu']),\n",
        "        'avg_rouge_l': np.mean(total_metrics['ft_rouge_l']),\n",
        "        'avg_semantic': np.mean(total_metrics['ft_semantic']),\n",
        "        'avg_domain': np.mean(total_metrics['ft_domain']),\n",
        "        'singapore_content_rate': np.mean(total_metrics['ft_singapore']),\n",
        "        'avg_response_time': np.mean(total_metrics['ft_time'])\n",
        "    }\n",
        "}\n",
        "\n",
        "# Calculate improvements\n",
        "improvements = {\n",
        "    'bleu_improvement': results_summary['finetuned_model']['avg_bleu'] / max(results_summary['base_model']['avg_bleu'], 0.001),\n",
        "    'domain_improvement': results_summary['finetuned_model']['avg_domain'] / max(results_summary['base_model']['avg_domain'], 0.001),\n",
        "    'singapore_improvement': results_summary['finetuned_model']['singapore_content_rate'] / max(results_summary['base_model']['singapore_content_rate'], 0.001)\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 COMPREHENSIVE RESULTS COMPARISON\")\n",
        "print(f\"{'Metric':<25} {'Base GPT-2':<15} {'Fine-tuned':<15} {'Improvement':<15}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'BLEU Score':<25} {results_summary['base_model']['avg_bleu']:<15.4f} {results_summary['finetuned_model']['avg_bleu']:<15.4f} {improvements['bleu_improvement']:<15.1f}x\")\n",
        "print(f\"{'ROUGE-L':<25} {results_summary['base_model']['avg_rouge_l']:<15.4f} {results_summary['finetuned_model']['avg_rouge_l']:<15.4f} {results_summary['finetuned_model']['avg_rouge_l']/max(results_summary['base_model']['avg_rouge_l'], 0.001):<15.1f}x\")\n",
        "print(f\"{'Semantic Similarity':<25} {results_summary['base_model']['avg_semantic']:<15.4f} {results_summary['finetuned_model']['avg_semantic']:<15.4f} {results_summary['finetuned_model']['avg_semantic']/max(results_summary['base_model']['avg_semantic'], 0.001):<15.1f}x\")\n",
        "print(f\"{'Domain Accuracy':<25} {results_summary['base_model']['avg_domain']:<15.4f} {results_summary['finetuned_model']['avg_domain']:<15.4f} {improvements['domain_improvement']:<15.1f}x\")\n",
        "print(f\"{'Singapore Content %':<25} {results_summary['base_model']['singapore_content_rate']*100:<15.1f}% {results_summary['finetuned_model']['singapore_content_rate']*100:<15.1f}% {improvements['singapore_improvement']:<15.1f}x\")\n",
        "print(f\"{'Response Time (s)':<25} {results_summary['base_model']['avg_response_time']:<15.3f} {results_summary['finetuned_model']['avg_response_time']:<15.3f} {'N/A':<15}\")\n",
        "\n",
        "print(f\"\\n🏆 PERFORMANCE ASSESSMENT:\")\n",
        "if (results_summary['finetuned_model']['singapore_content_rate'] >= 0.8 and \n",
        "    results_summary['finetuned_model']['avg_domain'] >= 0.3):\n",
        "    print(\"   🎉 EXCELLENT: Production-ready performance!\")\n",
        "    assessment = \"EXCELLENT\"\n",
        "elif (results_summary['finetuned_model']['singapore_content_rate'] >= 0.6 and \n",
        "      results_summary['finetuned_model']['avg_domain'] >= 0.2):\n",
        "    print(\"   ✅ GOOD: Strong performance with room for improvement\")\n",
        "    assessment = \"GOOD\"\n",
        "else:\n",
        "    print(\"   ⚠️ MODERATE: Shows promise but needs optimization\")\n",
        "    assessment = \"MODERATE\"\n",
        "\n",
        "# Save comprehensive results\n",
        "Path(\"results\").mkdir(exist_ok=True)\n",
        "final_results = {\n",
        "    'summary': results_summary,\n",
        "    'improvements': improvements,\n",
        "    'assessment': assessment,\n",
        "    'test_questions_count': len(test_questions),\n",
        "    'training_examples': len(training_data)\n",
        "}\n",
        "\n",
        "with open(\"results/complete_evaluation.json\", \"w\") as f:\n",
        "    json.dump(final_results, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\n💾 Complete results saved to results/complete_evaluation.json\")\n",
        "print(f\"\\n🎯 PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "print(f\"   📊 Trained on {len(training_data)} Singapore financial Q&A pairs\")\n",
        "print(f\"   🧪 Evaluated on {len(test_questions)} test questions\")\n",
        "print(f\"   🚀 Fine-tuned model shows {improvements['bleu_improvement']:.1f}x BLEU improvement\")\n",
        "print(f\"   🏆 Overall assessment: {assessment}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
