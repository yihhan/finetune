# üîç Analysis: Why Large Dataset Training Failed

## ‚ùå **What Went Wrong:**

### 1. **Nonsensical Responses:**
- "Maternal Surgery of Melbourne" for "What does MAS stand for?"
- "Shanghai" for Singapore currency
- "Central bank of Scotland" for Singapore bank regulation
- Completely unrelated financial gibberish

### 2. **Root Causes:**

#### **A. Model Architecture Issue:**
- **Flan-T5-base** might be too large and complex for our LoRA approach
- The base model's pre-training is interfering with our domain adaptation
- LoRA might not be sufficient to override the base model's knowledge

#### **B. Training Data Quality:**
- Our generated data might be too repetitive (62 identical samples per topic)
- Lack of diversity in question-answer patterns
- Mock data doesn't reflect real regulatory complexity

#### **C. Training Parameters:**
- **3 epochs** might be too few for 496 samples
- **Learning rate 1e-4** might be too conservative
- **Batch size 4** might not provide enough gradient updates

#### **D. LoRA Configuration:**
- **r=32, alpha=64** might still be insufficient for large dataset
- Need more aggressive parameters or full fine-tuning

## ‚úÖ **Solutions to Try:**

### **Option 1: Full Fine-Tuning (Recommended)**
```python
# Remove LoRA entirely, train all parameters
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
# No PEFT/LoRA - direct fine-tuning
```

### **Option 2: Much More Aggressive LoRA**
```python
lora_config = LoraConfig(
    r=64,  # Double the rank
    lora_alpha=128,  # Double the alpha
    target_modules=["q", "v", "k", "o", "wi", "wo"],  # All linear layers
    lora_dropout=0.1,
    bias="all",  # Train bias terms too
)
```

### **Option 3: Smaller Base Model**
```python
model_name = "google/flan-t5-small"  # Much smaller, easier to fine-tune
```

### **Option 4: Better Training Data**
- Use real MAS documents instead of mock data
- More diverse question patterns
- Fewer repetitions, more unique samples

### **Option 5: More Aggressive Training**
```python
TrainingArguments(
    num_train_epochs=10,  # Much more training
    learning_rate=5e-4,   # Higher learning rate
    per_device_train_batch_size=2,  # Smaller batch, more updates
)
```

## üéØ **Recommended Next Step:**

**Try Option 1 (Full Fine-Tuning) with Flan-T5-small:**
- Remove LoRA completely
- Use smaller model (easier to fine-tune)
- More aggressive training parameters
- This should give us proper Singapore financial responses

The current approach proves fine-tuning *works* (different responses), but we need to fix the *quality* issue.
